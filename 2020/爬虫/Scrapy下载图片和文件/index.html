<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"yoursite.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:15,onmobile:!0},copycode:{enable:!0,show_result:!1,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!0,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><link rel="stylesheet" href="/js/prism/prism.css"><meta name="description" content="当我们的scrapy项目需要爬取媒体内容时，可以使用以下两条scrapy模块自带的pipeline管道进行下载处理，都包含了两个默认特殊字段：  对于文件：scrapy.pipelines.files.FilesPipeline item[&#39;file_urls&#39;] - 下载链接列表 item[&#39;files&#39;] - 管道处理结果列表（每个url的信息字典）   对于图"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy框架爬取文件和图片"><meta property="og:url" content="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/Scrapy%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E5%92%8C%E6%96%87%E4%BB%B6/index.html"><meta property="og:site_name" content="苑东旭的博客"><meta property="og:description" content="当我们的scrapy项目需要爬取媒体内容时，可以使用以下两条scrapy模块自带的pipeline管道进行下载处理，都包含了两个默认特殊字段：  对于文件：scrapy.pipelines.files.FilesPipeline item[&#39;file_urls&#39;] - 下载链接列表 item[&#39;files&#39;] - 管道处理结果列表（每个url的信息字典）   对于图"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-03-03T16:00:00.000Z"><meta property="article:modified_time" content="2020-09-10T14:29:15.643Z"><meta property="article:author" content="yuandongxu"><meta property="article:tag" content="爬虫"><meta property="article:tag" content="Scrapy"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/Scrapy%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E5%92%8C%E6%96%87%E4%BB%B6/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Scrapy框架爬取文件和图片 | 苑东旭的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">苑东旭的博客</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">笔记·归纳·总结</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">12</span></a></li><li class="menu-item menu-item-时间线"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>时间线</a></li><li class="menu-item menu-item-工具"><a href="/tools" rel="section"><i class="fa fa-th fa-fw"></i>工具</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/Scrapy%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E5%92%8C%E6%96%87%E4%BB%B6/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/asset/av.jpg"><meta itemprop="name" content="yuandongxu"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="苑东旭的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Scrapy框架爬取文件和图片</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-03-04 00:00:00" itemprop="dateCreated datePublished" datetime="2020-03-04T00:00:00+08:00">2020-03-04</time> </span><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.7k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-tag"></i> </span><span class="post-meta-item-text">标签：</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/tags/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a> </span><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/tags/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>当我们的scrapy项目需要爬取媒体内容时，可以使用以下两条scrapy模块自带的pipeline管道进行下载处理，都包含了两个默认特殊字段：</p><ul><li>对于文件：<code>scrapy.pipelines.files.FilesPipeline</code><ul><li><code>item[&#39;file_urls&#39;]</code> - 下载链接列表</li><li><code>item[&#39;files&#39;]</code> - 管道处理结果列表（每个url的信息字典）</li></ul></li><li>对于图片：<code>scrapy.pipelines.images.ImagesPipeline</code><ul><li><code>item[&#39;image_urls&#39;]</code> - 下载链接列表</li><li><code>item[&#39;images&#39;]</code> - 管道处理结果列表（每个url的信息字典）</li></ul></li></ul><p>这些管道都实现了：</p><ul><li>避免重新下载最近下载的媒体</li><li>指定存储媒体的位置（本地、ftp位置、AmazonS3存储桶、Google云存储桶）</li></ul><p>imagespipeline作为filespipeline的子类，还实现了：</p><ul><li>将所有下载的图像转换为通用格式（JPG）和模式（RGB）</li><li>缩略图生成</li><li>图像尺寸过滤</li></ul><p>通过重写pipeline，还可以实现：</p><ul><li>自定义文件名称</li><li>自定义保存路径</li><li>爬取结果的处理</li></ul><p>参考：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjcmFweS5vcmcvZW4vbGF0ZXN0L3RvcGljcy9tZWRpYS1waXBlbGluZS5odG1s">scrapy document v2.3<i class="fa fa-external-link-alt"></i></span></p><a id="more"></a><hr><h2 id="1-媒体管道的使用"><a href="#1-媒体管道的使用" class="headerlink" title="1. 媒体管道的使用"></a>1. 媒体管道的使用</h2><p>（1）先在settings.py中启用管道并配置相关的存储位置。</p><pre><code class="line-numbers language-python">ITEM_PIPELINES = &#123;
    &#39;scrapy.pipelines.images.ImagesPipeline&#39;: 1
    &#125;
IMAGES_STORE = &#39;/data/scrapy/images&#39;</code></pre><p>（2）配置item字段</p><pre><code class="line-numbers language-python">import scrapy

class MyItem(scrapy.Item):
    # ... other item fields ...
    image_urls = scrapy.Field()        # 源文件链接
    images = scrapy.Field()        # 管道处理后的结果（爬取结果）</code></pre><p>（3）编码spider，将图像链接列表保存到<code>item[&#39;image_urls&#39;]</code>中</p><p>（4）爬取</p><h3 id="下载结果示例"><a href="#下载结果示例" class="headerlink" title="下载结果示例"></a>下载结果示例</h3><p>管道默认使用URL的SHA1 Hash值作为文件的名称。</p><p>比如说需要下载的图片URL为：<span class="exturl" data-url="aHR0cDovL3d3dy5leGFtcGxlLmNvbS9pbWFnZS5qcGc=">http://www.example.com/image.jpg<i class="fa fa-external-link-alt"></i></span></p><p>它的SHA1 hash为：<code>3afec3b4765f8f0a07b78f98c07b83f013567a0a</code></p><p>最终下载得到的文件为：<code>/data/scrapy/images/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg</code></p><hr><h2 id="2-媒体管道的配置项"><a href="#2-媒体管道的配置项" class="headerlink" title="2. 媒体管道的配置项"></a>2. 媒体管道的配置项</h2><p>在项目settings.py中进行配置</p><h3 id="爬取结果保存位置"><a href="#爬取结果保存位置" class="headerlink" title="爬取结果保存位置"></a>爬取结果保存位置</h3><pre><code class="line-numbers language-bash">IMAGES_STORE = &#39;/path/to/valid/dir&#39;        # 图像保存位置
FILES_STORE = &#39;/path/to/valid/dir&#39;        # 文件保存位置</code></pre><p>FILES_STORE和IMAGES_STORE可以指向一个FTP服务器，Scrapy会自动将文件上传到服务器，FILES_STORE和IMAGES_STORE应该以下列形式之一写入:</p><pre><code class="line-numbers language-bash">ftp://username:password@address:port/path
ftp://address:port/path</code></pre><p>如果未提供用户名和密码，则分别取自</p><pre><code class="line-numbers language-bash">FTP_USER = &#39;admin&#39;
FTP_PASSWORD = &#39;admin&#39;</code></pre><h3 id="自定义item字段名称"><a href="#自定义item字段名称" class="headerlink" title="自定义item字段名称"></a>自定义item字段名称</h3><p>配置该字段可以替换掉初始的<code>file_urls</code>与<code>file</code></p><pre><code class="line-numbers language-bash"># 对于文件管道
FILES_URLS_FIELD = &#39;field_name_for_your_files_urls&#39;
FILES_RESULT_FIELD = &#39;field_name_for_your_processed_files&#39;
# 对于图像管道
IMAGES_URLS_FIELD = &#39;field_name_for_your_images_urls&#39;
IMAGES_RESULT_FIELD = &#39;field_name_for_your_processed_images&#39;</code></pre><h3 id="文件过期"><a href="#文件过期" class="headerlink" title="文件过期"></a>文件过期</h3><p>为了避免重复下载最近下载过的文件、图片，默认90day</p><pre><code class="line-numbers language-bash"># 120 days of delay for files expiration
FILES_EXPIRES = 120
# 30 days of delay for images expiration
IMAGES_EXPIRES = 30</code></pre><p>对于重写的子类pipeline，配置项添加自定义的类名作为前缀</p><pre><code class="line-numbers language-bash">MYPIPELINECLASSNAME_FILES_EXPIRES = 180</code></pre><h3 id="图片管道的缩略图"><a href="#图片管道的缩略图" class="headerlink" title="图片管道的缩略图"></a>图片管道的缩略图</h3><p>图像管道可以自动创建下载图像的缩略图，要使用此功能，必须设置 <a target="_blank" rel="noopener" href="https://www.osgeo.cn/scrapy/topics/media-pipeline.html#std:setting-IMAGES_THUMBS"><code>IMAGES_THUMBS</code></a> 到一个字典，其中键是缩略图名称，值是它们的尺寸。</p><pre><code class="line-numbers language-bash">IMAGES_THUMBS = &#123;
    &#39;small&#39;: (50, 50),
    &#39;big&#39;: (270, 270),
&#125;</code></pre><p>使用此功能时，图像管道将保存缩略图到下列位置：</p><pre><code class="line-numbers language-bash">&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg
# &lt;size_name&gt; 是在 IMAGES_THUMBS 字典键 (small ， big 等）
# &lt;image_id&gt; 是 SHA1 hash 图像URL的
# 实例
&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg        # 原始
&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</code></pre><h3 id="图像尺寸过滤"><a href="#图像尺寸过滤" class="headerlink" title="图像尺寸过滤"></a>图像尺寸过滤</h3><p>使用图像管道时，可以过滤过小的图像，但不影响缩略图的生成。</p><pre><code class="line-numbers language-bash">IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110</code></pre><h3 id="下载失败处理"><a href="#下载失败处理" class="headerlink" title="下载失败处理"></a>下载失败处理</h3><p>管道默认忽略掉失败的下载请求，如果需要重新处理失败的请求，配置：</p><pre><code class="line-numbers language-bash">MEDIA_ALLOW_REDIRECTS = True</code></pre><h2 id="3-重写媒体管道类"><a href="#3-重写媒体管道类" class="headerlink" title="3. 重写媒体管道类"></a>3. 重写媒体管道类</h2><p>自定义类继承原管道并重写媒体管道中的方法可以对爬取中的细节进行处理</p><h3 id="class-scrapy-pipelines-files-FilesPipeline"><a href="#class-scrapy-pipelines-files-FilesPipeline" class="headerlink" title="class scrapy.pipelines.files.FilesPipeline"></a><em>class</em> <code>scrapy.pipelines.files.FilesPipeline</code></h3><h4 id="get-media-requests"><a href="#get-media-requests" class="headerlink" title="get_media_requests"></a><code>get_media_requests</code></h4><p><code>get_media_requests</code>(<em>item</em>, <em>info</em>)</p><p>因为item中保存下载链接的是一个列表字段，该方法用于从中分离单个链接并创建下载请求</p><pre><code class="line-numbers language-python">def get_media_requests(self, item, info):
    for file_url in item[&#39;file_urls&#39;]:
        yield scrapy.Request(file_url)</code></pre><p>下载完成后，结果将发送到 <a target="_blank" rel="noopener" href="https://www.osgeo.cn/scrapy/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.item_completed"><code>item_completed()</code></a> 方法，生成一个两元素的set<code>(success, file_info_or_error)</code></p><pre><code class="line-numbers language-bash">[(True,
  &#123;&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,
   &#39;path&#39;: &#39;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/files/product1.pdf&#39;,
   &#39;status&#39;: &#39;downloaded&#39;&#125;),
 (False,
  Failure(...))]</code></pre><ul><li><p><code>success</code> - True下载成功，False下载失败</p></li><li><p><code>file_info_or_error</code> is a dict containing the following keys (if success is <code>True</code>) or a <a target="_blank" rel="noopener" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html"><code>Failure</code></a> if there was a problem.</p><ul><li><p><code>url</code> - 文件的源URL链接. This is the url of the request returned from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests"><code>get_media_requests()</code></a> method.</p></li><li><p><code>path</code> - 保存路径（FILES_STORE + path）</p></li><li><p><code>checksum</code> - 数据的MD5</p></li><li><p><code>status</code> - 状态指示，scrapy&gt;2.2版本提供</p><p>It can be one of the following:</p><ul><li><code>downloaded</code> - file was downloaded.</li><li><code>uptodate</code> - file was not downloaded, as it was downloaded recently, according to the file expiration policy.</li><li><code>cached</code> - file was already scheduled for download, by another item sharing the same file.</li></ul></li></ul></li></ul><p>默认情况下 <a target="_blank" rel="noopener" href="https://www.osgeo.cn/scrapy/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests"><code>get_media_requests()</code></a> 方法返回 <code>None</code> ，这意味着该项目没有可下载的文件。</p><h4 id="file-path"><a href="#file-path" class="headerlink" title="file_path"></a><code>file_path</code></h4><p><code>file_path</code>(<em>self</em>, <em>request</em>, <em>response=None</em>, <em>info=None</em>)</p><p>（1）可以重写此方法以自定义每个文件的下载路径，默认情况下 <a target="_blank" rel="noopener" href="https://www.osgeo.cn/scrapy/topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"><code>file_path()</code></a> 方法返回 <code>full/&lt;request URL hash&gt;.&lt;extension&gt;</code> .</p><p>（2）重写以下载图片到files文件夹并保持原始的文件名称：即<code>https://example.com/a/b/c/foo.png --&gt; files/foo.png</code></p><pre><code class="line-numbers language-python">import os
from urllib.parse import urlparse

from scrapy.pipelines.files import FilesPipeline

class MyFilesPipeline(FilesPipeline):

    def file_path(self, request, response, info):
        return &#39;files/&#39; + os.path.basename(urlparse(request.url).path)</code></pre><h4 id="item-completed"><a href="#item-completed" class="headerlink" title="item_completed"></a><code>item_completed</code></h4><p><code>item_completed</code>(<em>results</em>, <em>item</em>, <em>info</em>)</p><p>当媒体管道处理完成了一项spider传递的item时（全部请求完毕，success or failed）会调用该方法，该方法用于为处理item的相关内容并返回item交给下一级管道</p><p>实例：将下载文件的路径传入item的<code>file_paths</code>字段中</p><pre><code class="line-numbers language-python">from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    file_paths = [x[&#39;path&#39;] for ok, x in results if ok]
    if not file_paths:
        raise DropItem(&quot;Item contains no files&quot;)
    item[&#39;file_paths&#39;] = file_paths
    return item</code></pre><h3 id="class-scrapy-pipelines-images-ImagesPipeline"><a href="#class-scrapy-pipelines-images-ImagesPipeline" class="headerlink" title="class scrapy.pipelines.images.``ImagesPipeline"></a><em>class</em> <code>scrapy.pipelines.images.``ImagesPipeline</code></h3><p>同上</p><h2 id="4-自定义的文件pipeline实例"><a href="#4-自定义的文件pipeline实例" class="headerlink" title="4. 自定义的文件pipeline实例"></a>4. 自定义的文件pipeline实例</h2><p>实现功能</p><ul><li>文件使用原名称重命名</li><li>按title文件夹归类</li><li>下载结果保存</li></ul><pre><code class="line-numbers language-python">import os, scrapy
from scrapy.pipelines.files import FilesPipeline
from urllib.parse import urlparse


class MyFilesPipeline(FilesPipeline):

    def get_media_requests(self, item, info):
        &quot;&quot;&quot;重命名&quot;&quot;&quot;
        for file_url in item[&#39;file_urls&#39;]:
            yield scrapy.Request(file_url, meta=&#123;&#39;item&#39;: item.copy()&#125;)

    def file_path(self, request, response=None, info=None):
        &quot;&quot;&quot;归类&quot;&quot;&quot;
        name = request.meta[&#39;item&#39;][&#39;name&#39;]
        title = request.meta[&#39;item&#39;][&#39;title&#39;]
        return &quot;%s/%s/%s&quot; % (name, title, os.path.basename(urlparse(request.url).path))

    def item_completed(self, results, item, info):
        &quot;&quot;&quot;下载结果处理&quot;&quot;&quot;
        data = &#123;&#125;
        for status, result in results:
            data[item[&#39;title&#39;]].append(result) if status else None
        file = &#39;./%s.json&#39; % item[&#39;name&#39;]  # 以spidername命名
        with open(file, &#39;w+&#39;) as f:
            f.writelines(data)
        return item</code></pre><p>item.py</p><pre><code class="line-numbers language-python">from scrapy import Field, Item

class FilespiderItem(Item):
    file_urls = Field()
    files = Field()
    name = Field()
    title = Field()</code></pre></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a> <a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/%E7%88%AC%E8%99%AB/scrapy%E5%91%BD%E4%BB%A4/" rel="prev" title="Scrapy命令行解析"><i class="fa fa-chevron-left"></i> Scrapy命令行解析</a></div><div class="post-nav-item"><a href="/2020/%E7%88%AC%E8%99%AB/Scrapyd%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/" rel="next" title="Scrapy项目部署工具scrapyd使用详解">Scrapy项目部署工具scrapyd使用详解 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">1. 媒体管道的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E7%BB%93%E6%9E%9C%E7%A4%BA%E4%BE%8B"><span class="nav-text">下载结果示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93%E7%9A%84%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="nav-text">2. 媒体管道的配置项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E7%BB%93%E6%9E%9C%E4%BF%9D%E5%AD%98%E4%BD%8D%E7%BD%AE"><span class="nav-text">爬取结果保存位置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89item%E5%AD%97%E6%AE%B5%E5%90%8D%E7%A7%B0"><span class="nav-text">自定义item字段名称</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E8%BF%87%E6%9C%9F"><span class="nav-text">文件过期</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E7%AE%A1%E9%81%93%E7%9A%84%E7%BC%A9%E7%95%A5%E5%9B%BE"><span class="nav-text">图片管道的缩略图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%B0%BA%E5%AF%B8%E8%BF%87%E6%BB%A4"><span class="nav-text">图像尺寸过滤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%A4%B1%E8%B4%A5%E5%A4%84%E7%90%86"><span class="nav-text">下载失败处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E9%87%8D%E5%86%99%E5%AA%92%E4%BD%93%E7%AE%A1%E9%81%93%E7%B1%BB"><span class="nav-text">3. 重写媒体管道类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-pipelines-files-FilesPipeline"><span class="nav-text">class scrapy.pipelines.files.FilesPipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#get-media-requests"><span class="nav-text">get_media_requests</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#file-path"><span class="nav-text">file_path</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#item-completed"><span class="nav-text">item_completed</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-pipelines-images-ImagesPipeline"><span class="nav-text">class scrapy.pipelines.images.&#96;&#96;ImagesPipeline</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%96%87%E4%BB%B6pipeline%E5%AE%9E%E4%BE%8B"><span class="nav-text">4. 自定义的文件pipeline实例</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="yuandongxu" src="/asset/av.jpg"><p class="site-author-name" itemprop="name">yuandongxu</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">33</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">12</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3l1YW5kb25neHU5Nw==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yuandongxu97"><i class="fab fa-github fa-fw"></i>GitHub</span> </span><span class="links-of-author-item"><a href="/431980110@qq.com" title="E-Mail → 431980110@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">yuandongxu</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">98k</span></div><script src="/js/prism/prism.js" async></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r,a=document.getElementsByTagName("link");if(0<a.length)for(i=0;i<a.length;i++)"canonical"==a[i].rel.toLowerCase()&&a[i].href&&(e=a[i].href);t=e?e.split(":")[0]:window.location.protocol.split(":")[0],e=e||window.location.href,window,n=e,r=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(n)||(o="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",r?(o+="?r="+encodeURIComponent(document.referrer),n&&(o+="&l="+n)):n&&(o+="?l="+n),(new Image).src=o)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script></body></html>