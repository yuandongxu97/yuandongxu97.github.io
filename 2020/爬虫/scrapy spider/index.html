<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"yoursite.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:15,onmobile:!0},copycode:{enable:!0,show_result:!1,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!0,lazyload:!1,pangu:!0,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!0,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/yuandongxu97/image-repository/app/prism.css"><meta name="description" content="我们在进行spider类的编写时，需要继承 scrapy.Spider类并对其中的方法进行重写，Spider类定义了对目标（URLorURLS）的处理逻辑，流程为：  首先，生成最初爬取请求，有两种设定方法  start_urls类属性：这是一个url的列表，scrapy会根据调度器的指定循环生成URL的请求，默认回调 parse方法解析response start_requsts方法：源码中这个"><meta property="og:type" content="article"><meta property="og:title" content="【Scrapy框架笔记】 spider基类与Spider模板类"><meta property="og:url" content="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/scrapy%20spider/index.html"><meta property="og:site_name" content="苑东旭的博客"><meta property="og:description" content="我们在进行spider类的编写时，需要继承 scrapy.Spider类并对其中的方法进行重写，Spider类定义了对目标（URLorURLS）的处理逻辑，流程为：  首先，生成最初爬取请求，有两种设定方法  start_urls类属性：这是一个url的列表，scrapy会根据调度器的指定循环生成URL的请求，默认回调 parse方法解析response start_requsts方法：源码中这个"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2020-02-04T16:00:00.000Z"><meta property="article:modified_time" content="2020-02-04T16:00:00.000Z"><meta property="article:author" content="yuandongxu"><meta property="article:tag" content="爬虫"><meta property="article:tag" content="Scrapy"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/scrapy%20spider/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>【Scrapy框架笔记】 spider基类与Spider模板类 | 苑东旭的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">苑东旭的博客</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">笔记·归纳·总结</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">13</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>时间线<span class="badge">38</span></a></li><li class="menu-item menu-item-工具"><a href="/tools" rel="section"><i class="fa fa-code fa-fw"></i>工具</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/%E7%88%AC%E8%99%AB/scrapy%20spider/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://cdn.jsdelivr.net/gh/yuandongxu97/image-repository/app/av.jpg"><meta itemprop="name" content="yuandongxu"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="苑东旭的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">【Scrapy框架笔记】 spider基类与Spider模板类</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-05T00:00:00+08:00">2020-02-05</time> </span><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.2k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-tag"></i> </span><span class="post-meta-item-text">标签：</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/tags/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a> </span><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/tags/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>我们在进行spider类的编写时，需要继承 <code>scrapy.Spider</code>类并对其中的方法进行重写，Spider类定义了对目标（URLorURLS）的处理逻辑，流程为：</p><ul><li><p>首先，生成最初爬取请求，有两种设定方法</p><ul><li><code>start_urls</code>类属性：这是一个url的列表，scrapy会根据调度器的指定循环生成URL的请求，默认回调 <code>parse</code>方法解析response</li><li><code>start_requsts</code>方法：源码中这个方法用于处理start_urls，可以重写这个方法改写请求的内容（更改headers、添加clouflare cookies等）</li></ul></li><li><p>然后，使用解析函数对response进行处理，</p><ul><li>获取爬取目标：横向的 <code>下一页</code>或者纵向的 <code>详情页</code></li><li>有效数据提取：存入<code>item</code>或交给<code>Request.meta</code></li></ul></li><li><p>最后，使用<code>yield item</code>将数据交给pipeline组件或者Feed进行导出</p></li></ul><p>参考：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjcmFweS5vcmcvZW4vbGF0ZXN0L3RvcGljcy9jb21tYW5kcy5odG1s">scrapy document v2.1<i class="fa fa-external-link-alt"></i></span></p><a id="more"></a><h2 id="scrapy-Spider类分析"><a href="#scrapy-Spider类分析" class="headerlink" title="scrapy.Spider类分析"></a>scrapy.Spider类分析</h2><p>我们编写的spider都继承于scrapy.Spider类</p><pre><code class="line-numbers language-python">import scrapy
from filespider.items import FileSpiderItem


class MySpider(scrapy.Spider):
    name = &#39;&#39;
    allow_domains = []
    start_urls = []
    custom_settings = &#123;&#125;

    def start_requests(self):
        # do something
        yield scrapy.Request(url=&#39;&#39;, callback=self.parse)

    def parse(self, response, **kwargs):
        item = FileSpiderItem()
        yield item</code></pre><h3 id="类属性"><a href="#类属性" class="headerlink" title="类属性"></a>类属性</h3><h4 id="name"><a href="#name" class="headerlink" title="name"></a>name</h4><p>spider标识，一个项目中每个spider的name属性必须是唯一的</p><pre><code class="line-numbers language-bash">scrapy list        # 获取项目中的spider列表
scrapy crawl &lt;name&gt;        # 运行name名称的spider</code></pre><h4 id="allowed-domains"><a href="#allowed-domains" class="headerlink" title="allowed_domains"></a>allowed_domains</h4><p>允许爬行域，如果配置为<code>allowed_domains=[&#39;movie.douban.com&#39;]</code>，那么：</p><ul><li>对<code>https://movie.douban.com/xxx</code>的请求会执行</li><li>对<code>https://music.douban.com/xxx</code>的请求会被抛弃</li></ul><h4 id="start-urls"><a href="#start-urls" class="headerlink" title="start_urls"></a>start_urls</h4><p>爬取的起点URL列表</p><h4 id="custom-settings"><a href="#custom-settings" class="headerlink" title="custom_settings"></a>custom_settings</h4><p>爬取配置，优先级高于settings.py，会在实例化前更新配置</p><h4 id="crawler"><a href="#crawler" class="headerlink" title="crawler"></a>crawler</h4><h4 id="settings-配置实例"><a href="#settings-配置实例" class="headerlink" title="settings    配置实例"></a>settings 配置实例</h4><h3 id="类方法"><a href="#类方法" class="headerlink" title="类方法"></a>类方法</h3><h4 id="from-crawler-crawler-args-kwargs"><a href="#from-crawler-crawler-args-kwargs" class="headerlink" title="from_crawler(crawler, args*, *kwargs*)"></a><code>from_crawler</code>(<em>crawler</em>, <strong>args*, *</strong>kwargs*)</h4><p>相当于spider类的<code>__init__</code>方法，一般不需要重写</p><h4 id="start-requests"><a href="#start-requests" class="headerlink" title="start_requests()"></a><code>start_requests</code>()</h4><p>这是一个生成器方法，最后必须返回iterable对象（<code>yield xxx</code>）</p><p>默认实现中对start_urls列表中的每个URL生成 <code>Request(url, dont_filter=True)</code></p><p><strong>重写实例：</strong>POST登陆</p><pre><code class="line-numbers language-python">class MySpider(scrapy.Spider):
    name = &#39;myspider&#39;

    def start_requests(self):
        return [scrapy.FormRequest(&quot;http://www.example.com/login&quot;,
                                   formdata=&#123;&#39;user&#39;: &#39;john&#39;, &#39;pass&#39;: &#39;secret&#39;&#125;,
                                   callback=self.logged_in)]

    def logged_in(self, response):
        # here you would extract links to follow and return Requests for
        # each of them, with another callback
        pass</code></pre><h4 id="parse-response"><a href="#parse-response" class="headerlink" title="parse(response)"></a><code>parse</code>(<em>response</em>)</h4><p>负责处理回调该函数的函数生成的请求的响应（···，其实就是使用callback的对象的响应）</p><p>重写后主要用于数据解析和URL获取</p><pre><code class="line-numbers language-python">def parse(self, response):
    for h3 in response.xpath(&#39;//h3&#39;).getall():
        yield &#123;&quot;title&quot;: h3&#125;
    for href in response.xpath(&#39;//a/@href&#39;).getall():
        yield scrapy.Request(response.urljoin(href), self.parse)</code></pre><p>必须返回如下三者之一：</p><ul><li><code>scrapy.Request</code>对象</li><li>dict字典对象</li><li>Item实例</li></ul><h4 id="log-message-level-component"><a href="#log-message-level-component" class="headerlink" title="log(message[, level, component])"></a><code>log</code>(<em>message</em>[, <em>level</em>, <em>component</em>])</h4><p>spider的logger，保持向后兼容性</p><pre><code class="line-numbers language-python">def parse_item(self, response):
        self.logger.info(&#39;Hi, this is an item page! %s&#39;, response.url)</code></pre><h4 id="closed-reason"><a href="#closed-reason" class="headerlink" title="closed(reason)"></a><code>closed</code>(<em>reason</em>)</h4><p>spider关闭时调用。此方法为 <code>spider_closed</code>信号。</p><h2 id="Spider类对crawl命令参数（-a）的解析"><a href="#Spider类对crawl命令参数（-a）的解析" class="headerlink" title="Spider类对crawl命令参数（-a）的解析"></a>Spider类对crawl命令参数（-a）的解析</h2><p>在<code>scrapy crawl</code>时可以使用<code>-a xxx=xxx</code>指定参数，以动态改变spider功能</p><h3 id="原理与注意"><a href="#原理与注意" class="headerlink" title="原理与注意"></a>原理与注意</h3><p>手册中对该功能的举例，可以看到这个例子巧妙的在spider的启动阶段改变了源URL列表</p><pre><code class="line-numbers language-bash">scrapy crawl myspider -a category=electronics</code></pre><pre><code class="line-numbers language-python">import scrapy

class MySpider(scrapy.Spider):
    name = &#39;myspider&#39;

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = [&#39;http://www.example.com/categories/%s&#39; % category]</code></pre><p>注意参数传入的只是<strong>字符串</strong>，要设置为属性必须使用<code>ast.literal_eval</code> 或 <code>json.loads</code>进行转换</p><h3 id="可直接使用接口"><a href="#可直接使用接口" class="headerlink" title="可直接使用接口"></a>可直接使用接口</h3><pre><code class="line-numbers language-bash">scrapy crawl myspider -a user_agent=mybot        # UserAgentMiddleware
scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword        # HttpAuthMiddleware</code></pre><h2 id="scrapy预设Spider子类"><a href="#scrapy预设Spider子类" class="headerlink" title="scrapy预设Spider子类"></a>scrapy预设Spider子类</h2><p>scrapy根据爬取过程中经常用到的功能预设了一些Spider类的子类，可用template：</p><ul><li>默认：<code>scrapy.Spider</code></li><li>CrawlSpider：class <code>scrapy.spiders.CrawlSpider</code></li><li>XMLFeedSpider：class <code>scrapy.spiders.XMLFeedSpider</code></li><li>CSVFeedSpider：class <code>scrapy.spiders.CSVFeedSpider</code></li><li>SitemapSpider：class <code>scrapy.spiders.SitemapSpider</code></li></ul><pre><code class="line-numbers language-bash">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;
scrapy genspider [--template=template] &lt;name&gt; &lt;domain&gt;</code></pre><h3 id="class-scrapy-spiders-CrawlSpider"><a href="#class-scrapy-spiders-CrawlSpider" class="headerlink" title="class scrapy.spiders.CrawlSpider"></a><em>class</em> <code>scrapy.spiders.CrawlSpider</code></h3><p>主要使用LinkExtractor方法模板化匹配并提取网页上的链接并生成请求，比如说爬取<strong>起点小说网</strong>，相对于父类，增加了：</p><ul><li>rules属性：爬取规则实例列表，这些实例由 <code>scrapy.spiders.Rule</code>爬行规则类生成</li><li>parse_start_url(response)方法：用于解析初始响应，返回值类似parse函数</li></ul><h4 id="scrapy-spiders-Rule-爬行规则类"><a href="#scrapy-spiders-Rule-爬行规则类" class="headerlink" title="scrapy.spiders.Rule 爬行规则类"></a>scrapy.spiders.Rule 爬行规则类</h4><pre><code class="line-numbers language-python">class scrapy.spiders.Rule(
    link_extractor, 
    callback=None, 
    cb_kwargs=None, 
    follow=None, 
    process_links=None, 
    process_request=None
)</code></pre><ul><li><p>link_extractor</p><p>是一个 <code>LinkExtractor</code> 对象，定义如何从每个已爬网页提取链接，每个生成的链接将用于生成 <code>Request</code> 对象，链接的文本保存在 <code>response.meta[&#39;link_text&#39;]</code></p></li><li><p>callback：指定该条匹配实例的解析函数</p></li></ul><h4 id="栗子"><a href="#栗子" class="headerlink" title="栗子"></a>栗子</h4><p>手册栗子很形象</p><pre><code class="line-numbers language-python">import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = &#39;example.com&#39;
    allowed_domains = [&#39;example.com&#39;]
    start_urls = [&#39;http://www.example.com&#39;]

    rules = (
        # Extract links matching &#39;category.php&#39; (but not matching &#39;subsection.php&#39;)
        # and follow links from them (since no callback means follow=True by default).
        Rule(LinkExtractor(allow=(&#39;category\.php&#39;, ), deny=(&#39;subsection\.php&#39;, ))),

        # Extract links matching &#39;item.php&#39; and parse them with the spider&#39;s method parse_item
        Rule(LinkExtractor(allow=(&#39;item\.php&#39;, )), callback=&#39;parse_item&#39;),
    )

    def parse_item(self, response):
        self.logger.info(&#39;Hi, this is an item page! %s&#39;, response.url)
        item = scrapy.Item()
        item[&#39;id&#39;] = response.xpath(&#39;//td[@id=&quot;item_id&quot;]/text()&#39;).re(r&#39;ID: (\d+)&#39;)
        item[&#39;name&#39;] = response.xpath(&#39;//td[@id=&quot;item_name&quot;]/text()&#39;).get()
        item[&#39;description&#39;] = response.xpath(&#39;//td[@id=&quot;item_description&quot;]/text()&#39;).get()
        item[&#39;link_text&#39;] = response.meta[&#39;link_text&#39;]
        return item</code></pre><h3 id="class-scrapy-spiders-XMLFeedSpider"><a href="#class-scrapy-spiders-XMLFeedSpider" class="headerlink" title="class  scrapy.spiders.XMLFeedSpider"></a>class <code>scrapy.spiders.XMLFeedSpider</code></h3><p>为解析XML而设计，没使用过···，看栗子</p><h4 id="栗子-1"><a href="#栗子-1" class="headerlink" title="栗子"></a>栗子</h4><pre><code class="line-numbers language-python">from scrapy.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = &#39;example.com&#39;
    allowed_domains = [&#39;example.com&#39;]
    start_urls = [&#39;http://www.example.com/feed.xml&#39;]
    iterator = &#39;iternodes&#39;  # This is actually unnecessary, since it&#39;s the default value
    itertag = &#39;item&#39;

    def parse_node(self, response, node):
        self.logger.info(&#39;Hi, this is a &lt;%s&gt; node!: %s&#39;, self.itertag, &#39;&#39;.join(node.getall()))

        item = TestItem()
        item[&#39;id&#39;] = node.xpath(&#39;@id&#39;).get()
        item[&#39;name&#39;] = node.xpath(&#39;name&#39;).get()
        item[&#39;description&#39;] = node.xpath(&#39;description&#39;).get()
        return item</code></pre><h3 id="class-scrapy-spiders-CSVFeedSpider"><a href="#class-scrapy-spiders-CSVFeedSpider" class="headerlink" title="class  scrapy.spiders.CSVFeedSpider"></a>class <code>scrapy.spiders.CSVFeedSpider</code></h3><p>处理CSV文件，与XMLFeedSpider相比该类用于行处理，每次迭代中调用<code>parse_row()</code>方法</p><h4 id="新增属性与方法"><a href="#新增属性与方法" class="headerlink" title="新增属性与方法"></a>新增属性与方法</h4><ul><li>delimiter：csv文件中每个字段<strong>分隔符</strong>字符串，默认为逗号 <code>&#39;,&#39;</code></li><li>quotechar：csv文件中每个字段包裹字符串，默认为双引号<code>&#39;&quot;&#39;</code></li><li>headers：字段列名</li><li>parse_row(response, row)：</li></ul><h4 id="栗子-2"><a href="#栗子-2" class="headerlink" title="栗子"></a>栗子</h4><pre><code class="line-numbers language-python">from scrapy.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = &#39;example.com&#39;
    allowed_domains = [&#39;example.com&#39;]
    start_urls = [&#39;http://www.example.com/feed.csv&#39;]
    delimiter = &#39;;&#39;
    quotechar = &quot;&#39;&quot;
    headers = [&#39;id&#39;, &#39;name&#39;, &#39;description&#39;]

    def parse_row(self, response, row):
        self.logger.info(&#39;Hi, this is a row!: %r&#39;, row)

        item = TestItem()
        item[&#39;id&#39;] = row[&#39;id&#39;]
        item[&#39;name&#39;] = row[&#39;name&#39;]
        item[&#39;description&#39;] = row[&#39;description&#39;]
        return item</code></pre><h3 id="class-scrapy-spiders-SitemapSpider"><a href="#class-scrapy-spiders-SitemapSpider" class="headerlink" title="class  scrapy.spiders.SitemapSpider"></a>class <code>scrapy.spiders.SitemapSpider</code></h3></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"><i class="fa fa-tag"></i> 爬虫</a> <a href="/tags/Scrapy/" rel="tag"><i class="fa fa-tag"></i> Scrapy</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2020/Docker/Docker%E9%83%A8%E7%BD%B2FileBrowser/" rel="prev" title="Linux使用Docker搭建FileBrowser主机文件浏览器"><i class="fa fa-chevron-left"></i> Linux使用Docker搭建FileBrowser主机文件浏览器</a></div><div class="post-nav-item"><a href="/2020/%E7%88%AC%E8%99%AB/scrapy%20%E5%91%BD%E4%BB%A4/" rel="next" title="【Scrapy框架笔记】 框架命令">【Scrapy框架笔记】 框架命令 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy-Spider%E7%B1%BB%E5%88%86%E6%9E%90"><span class="nav-text">scrapy.Spider类分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E5%B1%9E%E6%80%A7"><span class="nav-text">类属性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#name"><span class="nav-text">name</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#allowed-domains"><span class="nav-text">allowed_domains</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#start-urls"><span class="nav-text">start_urls</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#custom-settings"><span class="nav-text">custom_settings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#crawler"><span class="nav-text">crawler</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#settings-%E9%85%8D%E7%BD%AE%E5%AE%9E%E4%BE%8B"><span class="nav-text">settings 配置实例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="nav-text">类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#from-crawler-crawler-args-kwargs"><span class="nav-text">from_crawler(crawler, args*, *kwargs*)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#start-requests"><span class="nav-text">start_requests()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parse-response"><span class="nav-text">parse(response)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#log-message-level-component"><span class="nav-text">log(message[, level, component])</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#closed-reason"><span class="nav-text">closed(reason)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spider%E7%B1%BB%E5%AF%B9crawl%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0%EF%BC%88-a%EF%BC%89%E7%9A%84%E8%A7%A3%E6%9E%90"><span class="nav-text">Spider类对crawl命令参数（-a）的解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E7%90%86%E4%B8%8E%E6%B3%A8%E6%84%8F"><span class="nav-text">原理与注意</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E6%8E%A5%E5%8F%A3"><span class="nav-text">可直接使用接口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scrapy%E9%A2%84%E8%AE%BESpider%E5%AD%90%E7%B1%BB"><span class="nav-text">scrapy预设Spider子类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-spiders-CrawlSpider"><span class="nav-text">class scrapy.spiders.CrawlSpider</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scrapy-spiders-Rule-%E7%88%AC%E8%A1%8C%E8%A7%84%E5%88%99%E7%B1%BB"><span class="nav-text">scrapy.spiders.Rule 爬行规则类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%97%E5%AD%90"><span class="nav-text">栗子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-spiders-XMLFeedSpider"><span class="nav-text">class scrapy.spiders.XMLFeedSpider</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%97%E5%AD%90-1"><span class="nav-text">栗子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-spiders-CSVFeedSpider"><span class="nav-text">class scrapy.spiders.CSVFeedSpider</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B0%E5%A2%9E%E5%B1%9E%E6%80%A7%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="nav-text">新增属性与方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%97%E5%AD%90-2"><span class="nav-text">栗子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#class-scrapy-spiders-SitemapSpider"><span class="nav-text">class scrapy.spiders.SitemapSpider</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="yuandongxu" src="https://cdn.jsdelivr.net/gh/yuandongxu97/image-repository/app/av.jpg"><p class="site-author-name" itemprop="name">yuandongxu</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">38</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3l1YW5kb25neHU5Nw==" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yuandongxu97"><i class="fab fa-github fa-fw"></i>GitHub</span> </span><span class="links-of-author-item"><a href="/431980110@qq.com" title="E-Mail → 431980110@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2017 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">yuandongxu</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">108k</span></div><script src="https://cdn.jsdelivr.net/gh/yuandongxu97/image-repository/app/prism.js" async></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script><script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r,a=document.getElementsByTagName("link");if(0<a.length)for(i=0;i<a.length;i++)"canonical"==a[i].rel.toLowerCase()&&a[i].href&&(e=a[i].href);t=e?e.split(":")[0]:window.location.protocol.split(":")[0],e=e||window.location.href,window,n=e,r=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(n)||(o="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",r?(o+="?r="+encodeURIComponent(document.referrer),n&&(o+="&l="+n)):n&&(o+="?l="+n),(new Image).src=o)}()</script><script src="/js/local-search.js"></script><script>if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}</script></body></html>